文一：

# TCP/IP、Http、Socket的区别



　　网络由下往上分为

　　物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。

　　通过初步的了解，我知道IP协议对应于网络层，TCP协议对应于传输层，而HTTP协议对应于应用层，

　　三者从本质上来说没有可比性，

　　socket则是对TCP/IP协议的封装和应用(程序员层面上)。

　　也可以说，TPC/IP协议是传输层协议，主要解决数据如何在网络中传输，

　　而HTTP是应用层协议，主要解决如何包装数据。

　　关于TCP/IP和HTTP协议的关系，网络有一段比较容易理解的介绍：

　　“我们在传输数据时，可以只使用(传输层)TCP/IP协议，但是那样的话，如果没有应用层，便无法识别数据内容。

　　如果想要使传输的数据有意义，则必须使用到应用层协议。

　　应用层协议有很多，比如HTTP、FTP、TELNET等，也可以自己定义应用层协议。

　　WEB使用HTTP协议作应用层协议，以封装HTTP文本信息，然后使用TCP/IP做传输层协议将它发到网络上。”

　　而我们平时说的最多的socket是什么呢，实际上socket是对TCP/IP协议的封装，Socket本身并不是协议，而是一个调用接口(API)。

　　通过Socket，我们才能使用TCP/IP协议。

　　实际上，Socket跟TCP/IP协议没有必然的联系。

　　Socket编程接口在设计的时候，就希望也能适应其他的网络协议。

　　所以说，Socket的出现只是使得程序员更方便地使用TCP/IP协议栈而已，是对TCP/IP协议的抽象，

　　从而形成了我们知道的一些最基本的函数接口，比如create、listen、connect、accept、send、read和write等等。

　　网络有一段关于socket和TCP/IP协议关系的说法比较容易理解：

　　“TCP/IP只是一个协议栈，就像操作系统的运行机制一样，必须要具体实现，同时还要提供对外的操作接口。

　　这个就像操作系统会提供标准的编程接口，比如win32编程接口一样，

　　TCP/IP也要提供可供程序员做网络开发所用的接口，这就是Socket编程接口。”

　　关于TCP/IP协议的相关只是，用博大精深来讲我想也不为过，单单查一下网上关于此类只是的资料和书籍文献的数量就知道，

　　这个我打算会买一些经典的书籍(比如《TCP/IP详解：卷一、卷二、卷三》)进行学习，今天就先总结一些基于基于TCP/IP协议的应用和编程接口的知识，也就是刚才说了很多的HTTP和Socket。

　　CSDN上有个比较形象的描述：HTTP是轿车，提供了封装或者显示数据的具体形式;Socket是发动机，提供了网络通信的能力。

　　实际上，传输层的TCP是基于网络层的IP协议的，而应用层的HTTP协议又是基于传输层的TCP协议的，而Socket本身不算是协议，就像上面所说，它只是提供了一个针对TCP或者UDP编程的接口。

　　下面是一些经常在笔试或者面试中碰到的重要的概念，特在此做摘抄和总结。

　　一、什么是TCP连接的三次握手

　　第一次握手：客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认;

　　第二次握手：服务器收到syn包，必须确认客户的SYN(ack=j+1)，同时自己也发送一个SYN包(syn=k)，即SYN+ACK包，此时服务器进入SYN_RECV状态;

　　第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。

　　握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。

　　理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。

　　断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手”(过程就不细写了，就是服务器和客户端交互，最终确定断开)

　　二、利用Socket建立网络连接的步骤

　　建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。

　　套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。

　　1、服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。

　　2、客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。

　　为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。

　　3、连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户端，一旦客户端确认了此描述，双方就正式建立连接。

　　而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。

　　三、HTTP链接的特点

　　HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。

　　HTTP连接最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。

　　四、TCP和UDP的区别(考得最多。。快被考烂了我觉得- -\\)

　　1、TCP是面向链接的，虽然说网络的不安全不稳定特性决定了多少次握手都不能保证连接的可靠性，但TCP的三次握手在最低限度上(实际上也很大程度上保证了)保证了连接的可靠性;

　　而UDP不是面向连接的，UDP传送数据前并不与对方建立连接，对接收到的数据也不发送确认信号，发送端不知道数据是否会正确接收，当然也不用重发，所以说UDP是无连接的、不可靠的一种数据传输协议。

　　2、也正由于1所说的特点，使得UDP的开销更小数据传输速率更高，因为不必进行收发数据的确认，所以UDP的实时性更好。

　　知道了TCP和UDP的区别，就不难理解为何采用TCP传输协议的MSN比采用UDP的QQ传输文件慢了，但并不能说QQ的通信是不安全的，

　　因为程序员可以手动对UDP的数据收发进行验证，比如发送方对每个数据包进行编号然后由接收方进行验证啊什么的，

　　即使是这样，UDP因为在底层协议的封装上没有采用类似TCP的“三次握手”而实现了TCP所无法达到的传输效率。



文二：

# 网络编程

为了便于理解，本文先从具体的内容开始，也就是通过一个实例介绍一下网络编程是怎么回事。

本文将以TCP协议为例介绍网络编程和协议之前的关系。为了简单，便于理解，本文以Python为例进行介绍，如果不了解Python编程语言关系也不大，下面代码很容易理解。我们知道在网络通信中无论是BS架构还是CS架构，通常分为服务端和客户端，只不过BS架构中的浏览器就是客户端。因此，本文的示例也包含服务端和客户端2部分的代码。代码功能很简单，就是实现客户端和服务端发送字符串。

![img](b21c8701a18b87d6a190ec3bbd014b3c1e30fdd6.jpeg)图1 客户端服务端通信模型

这个代码清单是服务端的代码，这段代码的作用就是在服务端的某个端口建立监听，并等待客户端建立连接。完成连接建立后，等待客户端发送数据，并将数据回传给客户端。

\#!/usr/bin/env python3#-*- coding:utf-8 -*-from socket import *from time import ctimehost = ''port = 12345buffsize = 2048ADDR = (host,port)# 创建一个基于TCP协议的套接字tctime = socket(AF_INET,SOCK_STREAM)tctime.bind(ADDR)# 在指定的地址和端口监听tctime.listen(3)while True:print('Wait for connection ...') tctimeClient,addr = tctime.accept() print("Connection from :",addr) while True: data = tctimeClient.recv(buffsize).decode() if not data: break tctimeClient.send(('[%s] %s' % (ctime(),data)).encode()) tctimeClient.close()tctimeClient.close()

阅读服务端的代码可以看出主要包括，socket、bind、listen、accept、recv和send几个。其中值得关注的是listen和accept，两者分别用于监听端口和接受客户端的连接请求。

下面代码清单是客户端的实现，这里特别的地方是有一个connect函数，该函数实现与服务端建立连接。

\#!/usr/bin/env python3#-*- coding:utf-8 -*-from socket import *HOST ='localhost'PORT = 12345BUFFSIZE=2048ADDR = (HOST,PORT)tctimeClient = socket(AF_INET,SOCK_STREAM)tctimeClient.connect(ADDR)while True:data = input(">") if not data: break tctimeClient.send(data.encode()) data = tctimeClient.recv(BUFFSIZE).decode() if not data: break print(data)tctimeClient.close()

通过上述示例代码可以看出服务端通常是被动的，而客户端则要主动一些。服务端程序建立对某个端口的监听，等待客户端的连接请求。客户端向服务端发送连接请求，不出意外的情况下连接建立成功，这时客户端和服务端之前就可以互发数据了。当然，在实际生产环境中意外是经常的，因此从协议和接口层面，需要处理各种意外，本文在协议部分将详细介绍。

另外，本文实现了一个基本的客户端和服务端通信的程序，这个模式的通信在实际生产中几乎不再使用。在实际生产中为了提高数据传输和处理的效率，通常采用异步模式，这些内容超出了本文的介绍范围，后续文章会逐渐介绍。

**TCP协议详解**

前文说了网络协议是网络中不同计算机信息通信的语言，为了实现交互，这个语言就需要有一定的格式。本文以TCP协议为例进行介绍。

TCP协议是一个可靠的传输协议，其可靠性表现在2方面，一方面是保证数据包可以按照发送的顺序到达，另外一方面是保证数据包一定程度的正确性（后文详解为什么是一定程度上的正确性）。其可靠性的实现则基于2点技术，一点是具有一个CRC校验，这样如果数据包中的某些数据出现错误可以通过该校验和发现；另外一点是每个数据包都有一个序号，这样就能保证数据包的顺序性，如果出现错位的数据包可以请求重发。

既然说到了格式，那我们先看一下TCP数据包的数据格式。如下图是TCP数据包的格式，包括原端口、目的端口、序列号和标识位等等内容，内容有些多，看着可能有点眼花。但从大的方面理解，这个数据包其实只包含2部分内容，一个是包头，另外一个则是具体需要传输的数据。在TCP协议的控制逻辑中，包头起着最为关键的作用，它是TCP协议中诸如建立连接、断开连接、重传和错误校验等各种特性的基础。

![img](0d338744ebf81a4c4a5bcc9a6e23035d252da601.jpeg)图2 TCP数据包格式

包头的其它信息的含义都比较明了，本文仅仅介绍几个标志位（URG、ACK、PSH、RST、SYN和FIN）的含义。具体含义如下：

ACK: 确认序号有效。RST：重置连接SYN：发起一个新连接FIN：释放一个连接**连接的建立**TCP在具体传输数据之前需要建立连接。这里的连接并不是物理连接，物理连接基于底层的协议已经建立完成，而且TCP建立连接也是要假设底层连接已经成功，TCP的连接其实是一个虚拟的，逻辑的连接。简单粗暴的理解，就是客户端和服务端分别记录了各自接受到的数据包的序号，并且将自身设置为某种状态。在TCP协议中，连接的建立通常成为3次握手，从字面的概念可以看出，连接的建立需要经过3次确认的过程。

![img](7a899e510fb30f24bd8615d9719cb247ac4b03de.jpeg)图3 建立连接的3次握手

TCP协议3次握手的过程如图所示，初始状态客户端和服务端都处于关闭状态。主要过程分为3步：

客户端发送预连接数据包： TCP的连接是由客户端主动发起建立，客户端会发送一个数据包（报文）给服务端，需要注意的是数据包中的SYN标识位为1。我们前文已经介绍，如果SYN为1，则说明为建立连接的数据包。同时，在该数据包中包含一个请求序列号，该序列号也是建立连接的依据。服务端回复连接确认： 服务端确认可以建立连接（服务端不一定可以建立连接，因为系统中套接字的数量是有限的）的情况下会向客户端发送一个应答数据包。在应答数据包中会将ACK标志位设置为1，表示为服务端应答数据包。同时，在应答数据包中会设置请求序列号和应答序列号的值，具体参考图3.客户端回复连接确认： 最后，客户端再次发送一个连接确认数据包，告诉服务端连接建立成功。从上面流程可以看出，连接的建立需要经过多次交互，这就是我们日常中所说的建立连接是高成本的操作。在实际生产环境中，为了应对这个问题，会减少连接建立的频度，通常的做法是建立连接池，传输数据时直接从连接池中获取连接，而不是新建连接。

有人可能觉得可以对建立连接的过程进行优化，比如将客户端最后一次的确认取消掉，觉得这个没有卵用。对于正常情况确实没有多大的作用，这里主要是应对异常情况。因为网络拓扑是非常复杂的，特别是在广域网中，有着数不清的网络节点，因此会出现各种异常情况。因此，TCP协议在设计的时候必须要保证异常情况下的可靠性。

我们这里举一个例子，就是连接请求超时的情况。假设客户端向服务端发送一个连接请求，由于各种原因，请求一直没有到达服务端，因此服务端也就没有回复连接确认消息。客户端连接超时，因此客户端重新发送一个连接请求到服务端，这次比较顺利，很快到达了，并且顺利建立了连接。之后，前一个数据包经过长途跋涉最终还是到了服务端，服务端也向客户端发送了回复数据包，服务端认为连接是建立成功的，并且会维持连接。但客户端层面认为连接是超时的，因此将永远不会关闭该连接。这样就会造成服务端有残留的资源，从而造成服务端资源浪费，久而久之可能会导致服务端无新连接资源可用。

另外一个需要说明的是客户端和服务端的套接字都有相应的状态，而且状态会随着连接的不同阶段变化。初始状态都是CLOSE，最终连接建立成功后都是ESTABLISHED,具体变化过程如图3所示。后面本文会详细介绍状态变化情况。

**传输数据**完成连接建立之后，客户端和服务端就可以进行数据传输了。我们知道TCP是可靠的传输，那么传输的可靠性是通过什么来保证的呢？主要就是通过包头中的校验和、请求序列号和应答序列号（参考图2）。

TCP数据内容的可靠性是通过校验和保证的。TCP在发送数据时都会计算整个数据包的校验和，并存储在包头的校验和字段中。接收方会按照规则进行计算，从而确认接收到的数据是否是正确的。发送发计算校验和的流程大概如下：

把伪首部、TCP包头和TCP数据分为16为的字，并把TCP包头中的校验和字段置0用反码加法累加所有16位数字对计算结果去反，将其填充到TCP包头的校验和字段接收方将所有原码相加，高位叠加，如果全为1则表示数据正确，否则说明数据有错误。

TCP数据包顺序的可靠性是通过请求序列号和应答序列号保证的。在数据传输中的每个请求都会有一个请求序列号，而在接收方接收到数据后会发送一个应答序列号，这样发送方就能知道数据是否被正确接收，而接收方也能知道数据是否出现乱序，从而保证数据包的顺序性。

**断开连接**TCP关闭连接分为4步，称为4次挥手。连接的关闭不一定是在客户端发起，服务端也可以发起关闭连接。关闭连接的过程如下：

发起方发送一个FIN置位的数据包,用来请求关闭发送方到接收方的连接接收方发送一个应答，ACK标志位为1，确认关闭。此时完成了发起方到接收方的连接，也即发送方无法再向接收方发送数据，但接收方还可以向发送方发送数据。接收方数据传输完成后向发起方发送一个FIN为1的包，表示请求断开连接发起方回复一个ACK包，确认关闭成功

![img](b999a9014c086e064dbeb841b80118f00ad1cbb0.jpeg)图4 关闭连接流程示意图

TCP是全双工通信，因此关闭连接时需要双向关闭连接。首先是关闭发起方关闭本端的连接，然后是关闭接收方在收到发起方的关闭请求后，除了回复关闭应答外，还要确保数据传输完成后发起一个关闭连接的请求，保证双向同时关闭。





----



文三：

# 聊聊Linux 五种IO模型

上一篇[《聊聊同步、异步、阻塞与非阻塞》](https://www.jianshu.com/p/aed6067eeac9)已经通俗的讲解了，要理解同步、异步、阻塞与非阻塞重要的两个概念点了，没有看过的，建议先看这篇博文理解这两个概念点。在认知上，建立统一的模型。这样，大家在继续看本篇时，才不会理解有偏差。

那么，在正式开始讲Linux IO模型前，比如：同步IO和异步IO，阻塞IO和非阻塞IO分别是什么，到底有什么区别？不同的人在不同的上下文下给出的答案是不同的。所以先限定一下本文的上下文。

# 1 概念说明

在进行解释之前，首先要说明几个概念：

> 用户空间和内核空间
>
> 进程切换
>
> 进程的阻塞
>
> 文件描述符
>
> 缓存 IO

## 1.1 用户空间与内核空间

现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。`操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限`。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，`操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间`。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。

## 1.2 进程切换

为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。

从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：

> 1. 保存处理机上下文，包括程序计数器和其他寄存器。
> 2. 更新PCB信息。
> 3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。
> 4. 选择另一个进程执行，并更新其PCB。
> 5. 更新内存管理的数据结构。
> 6. 恢复处理机上下文。

注：总而言之就是很耗资源，具体的可以参考这篇文章：[进程切换](https://links.jianshu.com/go?to=http%3A%2F%2Fguojing.me%2Flinux-kernel-architecture%2Fposts%2Fprocess-switch%2F)。

## 1.3 进程的阻塞

正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。`当进程进入阻塞状态，是不占用CPU资源的`。

## 1.4 文件描述符fd

文件描述符（File descriptor）是计算机科学中的一个术语，`是一个用于表述指向文件的引用的抽象化概念`。

文件描述符在形式上是一个非负整数。实际上，`它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表`。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。

## 1.5 缓存 IO

`缓存 IO 又被称作标准 IO，大多数文件系统的默认 IO 操作都是缓存 IO`。在 Linux 的缓存 IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，`数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间`。

**缓存 IO 的缺点：**

`数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作`，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。

# 2 Linux IO模型

`网络IO的本质是socket的读取，socket在linux系统被抽象为流，IO可以理解为对流的操作`。刚才说了，对于一次IO访问（以read举例），`数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间`。所以说，当一个read操作发生时，它会经历两个阶段：

> 1. 第一阶段：等待数据准备 (Waiting for the data to be ready)。
> 2. 第二阶段：将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)。

对于socket流而言，

> 1. 第一步：通常涉及等待网络上的数据分组到达，然后被复制到内核的某个缓冲区。
> 2. 第二步：把数据从内核缓冲区复制到应用进程缓冲区。

网络应用需要处理的无非就是两大类问题，`网络IO，数据计算`。相对于后者，网络IO的延迟，给应用带来的性能瓶颈大于后者。网络IO的模型大致有如下几种：

> - **同步模型（synchronous IO）**
> - 阻塞IO（bloking IO）
> - 非阻塞IO（non-blocking IO）
> - 多路复用IO（multiplexing IO）
> - 信号驱动式IO（signal-driven IO）
> - **异步IO（asynchronous IO）**

**`注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。`**

在深入介绍Linux IO各种模型之前，让我们先来探索一下基本 Linux IO 模型的简单矩阵。如下图所示：

![img](2062729-49ac9eb7d4d2d019.png)

输入图片说明

每个 IO 模型都有自己的使用模式，它们对于特定的应用程序都有自己的优点。本节将简要对其一一进行介绍。`常见的IO模型有阻塞、非阻塞、IO多路复用，异步`。以一个生动形象的例子来说明这四个概念。周末我和女友去逛街，中午饿了，我们准备去吃饭。周末人多，吃饭需要排队，我和女友有以下几种方案。

## 2.1 同步阻塞 IO（blocking IO）

### 2.1.1 场景描述

> 我和女友点完餐后，不知道什么时候能做好，只好坐在餐厅里面等，直到做好，然后吃完才离开。女友本想还和我一起逛街的，但是不知道饭能什么时候做好，只好和我一起在餐厅等，而不能去逛街，直到吃完饭才能去逛街，中间等待做饭的时间浪费掉了。`这就是典型的阻塞`。

### 2.1.2 网络模型

`同步阻塞 IO 模型是最常用的一个模型，也是最简单的模型`。在linux中，`默认情况下所有的socket都是blocking`。它符合人们最常见的思考逻辑。`阻塞就是进程 "被" 休息, CPU处理其它进程去了`。

在这个IO模型中，用户空间的应用程序执行一个系统调用（recvform），这会导致应用程序阻塞，什么也不干，直到数据准备好，并且将数据从内核复制到用户进程，最后进程再处理数据，`在等待数据到处理数据的两个阶段`，整个进程都被阻塞。不能处理别的网络IO。`调用应用程序处于一种不再消费 CPU 而只是简单等待响应的状态`，因此从处理的角度来看，这是非常有效的。在调用recv()/recvfrom()函数时，发生在内核中等待数据和复制数据的过程，大致如下图：

![img](2062729-41550941d72be698.png)

输入图片说明

### 2.1.3 流程描述

当用户进程调用了recv()/recvfrom()这个系统调用，`kernel就开始了IO的第一个阶段：准备数据`（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。`第二个阶段：当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存`，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。

> 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。

**优点：**

> 1. 能够及时返回数据，无延迟；
> 2. 对内核开发者来说这是省事了；

**缺点：**

> 1. 对用户来说处于等待就要付出性能的代价了；

## 2.2 同步非阻塞 IO（nonblocking IO）

### 2.2.1 场景描述

> 我女友不甘心白白在这等，又想去逛商场，又担心饭好了。所以我们逛一会，回来询问服务员饭好了没有，来来回回好多次，饭都还没吃都快累死了啦。`这就是非阻塞`。需要不断的询问，是否准备好了。

### 2.2.2 网络模型

`同步非阻塞就是 “每隔一会儿瞄一眼进度条” 的轮询（polling）方式`。在这种模型中，`设备是以非阻塞的形式打开的`。这意味着 IO 操作不会立即完成，read 操作可能会返回一个错误代码，说明这个命令不能立即满足（EAGAIN 或 EWOULDBLOCK）。

在网络IO时候，非阻塞IO也会进行recvform系统调用，检查数据是否准备好，与阻塞IO不一样，"非阻塞将大的整片时间的阻塞分成N多的小的阻塞, 所以进程不断地有机会 '被' CPU光顾"。

`也就是说非阻塞的recvform系统调用调用之后，进程并没有被阻塞，内核马上返回给进程，如果数据还没准备好，此时会返回一个error`。进程在返回之后，可以干点别的事情，然后再发起recvform系统调用。重复上面的过程，循环往复的进行recvform系统调用。`这个过程通常被称之为轮询`。轮询检查内核数据，直到数据准备好，再拷贝数据到进程，进行数据处理。**`需要注意，拷贝数据整个过程，进程仍然是属于阻塞的状态`**。

在linux下，可以通过设置socket使其变为non-blocking。`当对一个non-blocking socket执行读操作时`，流程如图所示：

![img](2062729-69746d536c2781da.png)

输入图片说明

### 2.2.3 流程描述

当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。

> 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。

**同步非阻塞方式相比同步阻塞方式：**

> 优点：能够在等待任务完成的时间里干其他活了（包括提交其他任务，也就是 “后台” 可以有多个任务在同时执行）。
>
> 缺点：任务完成的响应延迟增大了，因为每过一段时间才去轮询一次read操作，而任务可能在两次轮询之间的任意时间完成。这会导致整体数据吞吐量的降低。

## 2.3 IO 多路复用（ IO multiplexing）

### 2.3.1 场景描述

> 与第二个方案差不多，餐厅安装了电子屏幕用来显示点餐的状态，这样我和女友逛街一会，回来就不用去询问服务员了，直接看电子屏幕就可以了。这样每个人的餐是否好了，都直接看电子屏幕就可以了，`这就是典型的IO多路复用`。

### 2.3.2 网络模型

由于同步非阻塞方式需要不断主动轮询，轮询占据了很大一部分过程，轮询会消耗大量的CPU时间，而 “后台” 可能有多个任务在同时进行，人们就想到了循环查询多个任务的完成状态，只要有任何一个任务完成，就去处理它。如果轮询不是进程的用户态，而是有人帮忙就好了。`那么这就是所谓的 “IO 多路复用”`。UNIX/Linux 下的 select、poll、epoll 就是干这个的（epoll 比 poll、select 效率高，做的事情是一样的）。

`IO多路复用有两个特别的系统调用select、poll、epoll函数`。select调用是内核级别的，select轮询相对非阻塞的轮询的区别在于---`前者可以等待多个socket，能实现同时对多个IO端口进行监听`，当其中任何一个socket的数据准好了，`就能返回进行可读`，`然后进程再进行recvform系统调用，将数据由内核拷贝到用户进程，当然这个过程是阻塞的`。select或poll调用之后，会阻塞进程，与blocking IO阻塞不同在于，`此时的select不是等到socket数据全部到达再处理, 而是有了一部分数据就会调用用户进程来处理`。如何知道有一部分数据到达了呢？`监视的事情交给了内核，内核负责数据到达的处理。也可以理解为"非阻塞"吧`。

`I/O复用模型会用到select、poll、epoll函数，这几个函数也会使进程阻塞，但是和阻塞I/O所不同的的，这两个函数可以同时阻塞多个I/O操作`。而且可以同时对多个读操作，多个写操作的I/O函数进行检测，直到有数据可读或可写时（注意不是全部数据可读或可写），才真正调用I/O操作函数。

对于多路复用，也就是轮询多个socket。`多路复用既然可以处理多个IO，也就带来了新的问题，多个IO之间的顺序变得不确定了`，当然也可以针对不同的编号。具体流程，如下图所示：

![img](2062729-d552e1e2ca381a08.png)

输入图片说明

### 2.3.3 流程描述

IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。`select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO`。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。

`当用户进程调用了select，那么整个进程会被block`，而同时，kernel会“监视”所有select负责的socket，`当任何一个socket中的数据准备好了，select就会返回`。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。

> 多路复用的特点是`通过一种机制一个进程能同时等待IO文件描述符`，内核监视这些文件描述符（套接字描述符），其中的任意一个进入读就绪状态，select， poll，epoll函数就可以返回。对于监视的方式，又可以分为 select， poll， epoll三种方式。

上面的图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。`因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)`。但是，`用select的优势在于它可以同时处理多个connection`。

所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。（select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）

在IO multiplexing Model中，`实际中，对于每一个socket，一般都设置成为non-blocking`，但是，如上图所示，整个用户的process其实是一直被block的。`只不过process是被select这个函数block，而不是被socket IO给block`。所以**`IO多路复用是阻塞在select，epoll这样的系统调用之上，而没有阻塞在真正的I/O系统调用如recvfrom之上。`**

在I/O编程过程中，`当需要同时处理多个客户端接入请求时，可以利用多线程或者I/O多路复用技术进行处理`。I/O多路复用技术`通过把多个I/O的阻塞复用到同一个select的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求`。与传统的多线程/多进程模型比，`I/O多路复用的最大优势是系统开销小`，系统不需要创建新的额外进程或者线程，也不需要维护这些进程和线程的运行，降底了系统的维护工作量，节省了系统资源，I/O多路复用的主要应用场景如下：

> 服务器需要同时处理多个处于监听状态或者多个连接状态的套接字。
>
> 服务器需要同时处理多种网络协议的套接字。

了解了前面三种IO模式，在用户进程进行系统调用的时候，**`他们在等待数据到来的时候，处理的方式不一样，直接等待，轮询，select或poll轮询`**，两个阶段过程：

> 第一个阶段有的阻塞，有的不阻塞，有的可以阻塞又可以不阻塞。
>
> 第二个阶段都是阻塞的。

**`从整个IO过程来看，他们都是顺序执行的，因此可以归为同步模型(synchronous)。都是进程主动等待且向内核检查状态。【此句很重要！！！】`**

`高并发的程序一般使用同步非阻塞方式而非多线程 + 同步阻塞方式`。要理解这一点，首先要扯到并发和并行的区别。比如去某部门办事需要依次去几个窗口，`办事大厅里的人数就是并发数，而窗口个数就是并行度`。也就是说`并发数是指同时进行的任务数（如同时服务的 HTTP 请求）`，而`并行数是可以同时工作的物理资源数量（如 CPU 核数）`。通过合理调度任务的不同阶段，并发数可以远远大于并行度，这就是区区几个 CPU 可以支持上万个用户并发请求的奥秘。在这种高并发的情况下，为每个任务（用户请求）创建一个进程或线程的开销非常大。`而同步非阻塞方式可以把多个 IO 请求丢到后台去，这就可以在一个进程里服务大量的并发 IO 请求`。

**注意：IO多路复用是同步阻塞模型还是异步阻塞模型，在此给大家分析下：**

> 此处仍然不太清楚的，强烈建议大家在细究[《聊聊同步、异步、阻塞与非阻塞》](https://links.jianshu.com/go?to=http%3A%2F%2Fmy.oschina.net%2Fxianggao%2Fblog%2F661085)中讲同步与异步的根本性区别，`同步是需要主动等待消息通知，而异步则是被动接收消息通知，通过回调、通知、状态等方式来被动获取消息`。`IO多路复用在阻塞到select阶段时，用户进程是主动等待并调用select函数获取数据就绪状态消息，并且其进程状态为阻塞`。所以，`把IO多路复用归为同步阻塞模式`。

## 2.4 信号驱动式IO（signal-driven IO）

信号驱动式I/O：首先我们允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。过程如下图所示：

![img](2062729-222408c76186fd8c.png)

输入图片说明

## 2.5 异步非阻塞 IO（asynchronous IO）

### 2.5.1 场景描述

> 女友不想逛街，又餐厅太吵了，回家好好休息一下。于是我们叫外卖，打个电话点餐，然后我和女友可以在家好好休息一下，饭好了送货员送到家里来。这就是典型的异步，只需要打个电话说一下，然后可以做自己的事情，饭好了就送来了。

### 2.5.2 网络模型

相对于同步IO，异步IO不是顺序执行。`用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情`。等到socket数据准备好了，内核直接复制数据给进程，`然后从内核向进程发送通知`。`IO两个阶段，进程都是非阻塞的`。

Linux提供了AIO库函数实现异步，但是用的很少。目前有很多开源的异步IO库，例如libevent、libev、libuv。异步过程如下图所示：

![img](2062729-5283bdc9d033e956.png)

输入图片说明

### 2.5.3 流程描述

用户进程发起aio_read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，`首先它会立刻返回，所以不会对用户进程产生任何block`。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，`当这一切都完成之后，kernel会给用户进程发送一个signal或执行一个基于线程的回调函数来完成这次 IO 处理过程`，告诉它read操作完成了。

**在 Linux 中，通知的方式是 “信号”：**

> `如果这个进程正在用户态忙着做别的事（例如在计算两个矩阵的乘积），那就强行打断之，调用事先注册的信号处理函数`，这个函数可以决定何时以及如何处理这个异步任务。由于信号处理函数是突然闯进来的，因此跟中断处理程序一样，有很多事情是不能做的，因此保险起见，`一般是把事件 “登记” 一下放进队列，然后返回该进程原来在做的事`。
>
> `如果这个进程正在内核态忙着做别的事`，例如以同步阻塞方式读写磁盘，`那就只好把这个通知挂起来了，等到内核态的事情忙完了，快要回到用户态的时候，再触发信号通知`。
>
> `如果这个进程现在被挂起了，例如无事可做 sleep 了，那就把这个进程唤醒`，下次有 CPU 空闲的时候，就会调度到这个进程，触发信号通知。

异步 API 说来轻巧，做来难，这主要是对 API 的实现者而言的。Linux 的异步 IO（AIO）支持是 2.6.22 才引入的，还有很多系统调用不支持异步 IO。Linux 的异步 IO 最初是为数据库设计的，`因此通过异步 IO 的读写操作不会被缓存或缓冲，这就无法利用操作系统的缓存与缓冲机制`。

**`很多人把 Linux 的 O_NONBLOCK 认为是异步方式，但事实上这是前面讲的同步非阻塞方式。`**需要指出的是，虽然 Linux 上的 IO API 略显粗糙，但每种编程框架都有封装好的异步 IO 实现。操作系统少做事，把更多的自由留给用户，正是 UNIX 的设计哲学，也是 Linux 上编程框架百花齐放的一个原因。

从前面 IO 模型的分类中，我们可以看出 AIO 的动机：

> 同步阻塞模型需要在 IO 操作开始时阻塞应用程序。这意味着不可能同时重叠进行处理和 IO 操作。
>
> 同步非阻塞模型允许处理和 IO 操作重叠进行，但是这需要应用程序根据重现的规则来检查 IO 操作的状态。
>
> 这样就剩下异步非阻塞 IO 了，它允许处理和 IO 操作重叠进行，包括 IO 操作完成的通知。

IO多路复用除了需要阻塞之外，`select 函数所提供的功能（异步阻塞 IO）与 AIO 类似`。不过，`它是对通知事件进行阻塞，而不是对 IO 调用进行阻塞`。

## 2.6 关于异步阻塞

有时我们的 API 只提供异步通知方式，例如在 node.js 里，`但业务逻辑需要的是做完一件事后做另一件事`，例如数据库连接初始化后才能开始接受用户的 HTTP 请求。`这样的业务逻辑就需要调用者是以阻塞方式来工作`。

`为了在异步环境里模拟 “顺序执行” 的效果，就需要把同步代码转换成异步形式，这称为 CPS（Continuation Passing Style）变换`。BYVoid 大神的 [continuation.js](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FBYVoid%2Fcontinuation) 库就是一个 CPS 变换的工具。`用户只需用比较符合人类常理的同步方式书写代码，CPS 变换器会把它转换成层层嵌套的异步回调形式`。

![img](2062729-0d7cf290f0c4b88c.png)

输入图片说明

![img](2062729-199172d7d0ae818d.png)

输入图片说明

`另外一种使用阻塞方式的理由是降低响应延迟`。如果采用非阻塞方式，一个任务 A 被提交到后台，就开始做另一件事 B，但 B 还没做完，A 就完成了，这时要想让 A 的完成事件被尽快处理（比如 A 是个紧急事务），要么丢弃做到一半的 B，要么保存 B 的中间状态并切换回 A，任务的切换是需要时间的（不管是从磁盘载入到内存，还是从内存载入到高速缓存），这势必降低 A 的响应速度。`因此，对实时系统或者延迟敏感的事务，有时采用阻塞方式比非阻塞方式更好`。

# 3 五种IO模型总结

## 3.1 blocking和non-blocking区别

调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。

## 3.2 synchronous IO和asynchronous IO区别

在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的：

> A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;
>
> An asynchronous I/O operation does not cause the requesting process to be blocked;

`两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞`。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。

有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，`定义中所指的”IO operation”是指真实的IO操作`，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，`当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了`，在这段时间内，进程是被block的。

而asynchronous IO则不一样，`当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成`。在这整个过程中，进程完全没有被block。

**各个IO Model的比较如图所示：**

![img](2062729-a630f1d47b7ba148.png)

输入图片说明

通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。`在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check`，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。`它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知`。在此期间，`用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据`。



----



# 五大 IO 模型比较

前四种 I/O 模型的主要区别在于第一个阶段，而第二个阶段是一样的：将数据从内核复制到应用进程过程中，应用进程会被阻塞。
 [图片上传失败...(image-377ab2-1538122299754)]

#### blocking和non-blocking区别

调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。

#### synchronous IO和asynchronous IO区别

在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的：

> A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;

> An asynchronous I/O operation does not cause the requesting process to be blocked;

- 同步 I/O：应用进程在调用 recvfrom 操作时会阻塞。
- 异步 I/O：不会阻塞。

阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O 都是同步 I/O，虽然非阻塞式 I/O 和信号驱动 I/O 在等待数据阶段不会阻塞，但是在之后的将数据从内核复制到应用进程这个操作会阻塞。

### select，poll，epoll比较

select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。

#### select

select的调用过程如下所示：



![img](https:////upload-images.jianshu.io/upload_images/9166160-aa1d85d14c25e889.png?imageMogr2/auto-orient/strip|imageView2/2/w/632/format/webp)

image



（1）使用copy_from_user从用户空间拷贝fd_set到内核空间

（2）注册回调函数__pollwait

（3）遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）

（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。

（5）__pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。

（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。

（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。

（8）把fd_set从内核空间拷贝到用户空间。

##### 总结：

select的几大缺点：

（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大

（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大

（3）select支持的文件描述符数量太小了，默认是1024

#### poll

poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，其他的都差不多。

#### epoll

epoll既然是对select和poll的改进，就应该能避免上述的三个缺点。那epoll都是怎么解决的呢？在此之前，我们先看一下epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。

对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。

对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。

对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。

##### 总结

（1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。

（2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。

#### 应用场景

很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。

##### 1. select 应用场景

select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时要求更高的场景，比如核反应堆的控制。

select 可移植性更好，几乎被所有主流平台所支持。

##### 2. poll 应用场景

poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。

需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。

需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且epoll 的描述符存储在内核，不容易调试。

##### 3. epoll 应用场景

只需要运行在 Linux 平台上，并且有非常大量的描述符需要同时轮询，而且这些连接最好是长连接。

### web服务器设计模型

首先我们先明确并发和并行的概念。

#### 并发&并行

> 并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。
>  并行需要硬件支持，如多流水线或者多处理器。
>  操作系统通过引入进程和线程，使得程序能够并发运行。

对于web服务而言，并发是指同时进行的任务数（如同时服务的 HTTP 请求），而并行是可以同时工作的物理资源数量（如 CPU 核数）。

而针对并发IO而言，Reactor模型是一种常见的处理方式

#### Reactor模型

Reactor的中心思想是将所有要处理的I/O事件注册到一个中心I/O多路复用器上，同时主线程/进程阻塞在多路复用器上；一旦有I/O事件到来或是准备就绪(文件描述符或socket可读、写)，多路复用器返回并将事先注册的相应I/O事件分发到对应的处理器中。

Reactor是一种事件驱动机制，用“好莱坞原则”来形容Reactor再合适不过了：不要打电话给我们，我们会打电话通知你。
 Reactor模式与Observer模式在某些方面极为相似：当一个主体发生改变时，所有依属体都得到通知。不过，观察者模式与单个事件源关联，而反应器模式则与多个事件源关联 。

在Reactor模式中，有5个关键的参与者：

- **描述符（handle）**：由操作系统提供的资源，用于识别每一个事件，如Socket描述符、文件描述符、信号的值等。在Linux中，它用一个整数来表示。事件可以来自外部，如来自客户端的连接请求、数据等。事件也可以来自内部，如信号、定时器事件。
- **同步事件多路分离器（event demultiplexer）**：事件的到来是随机的、异步的，无法预知程序何时收到一个客户连接请求或收到一个信号。所以程序要循环等待并处理事件，这就是事件循环。在事件循环中，等待事件一般使用I/O复用技术实现。在linux系统上一般是select、poll、epol_waitl等系统调用，用来等待一个或多个事件的发生。I/O框架库一般将各种I/O复用系统调用封装成统一的接口，称为事件多路分离器。调用者会被阻塞，直到分离器分离的描述符集上有事件发生。
- **事件处理器（event handler）**：I/O框架库提供的事件处理器通常是由一个或多个模板函数组成的接口。这些模板函数描述了和应用程序相关的对某个事件的操作，用户需要继承它来实现自己的事件处理器，即具体事件处理器。因此，事件处理器中的回调函数一般声明为虚函数，以支持用户拓展。
- **具体的事件处理器（concrete event handler）**：是事件处理器接口的实现。它实现了应用程序提供的某个服务。每个具体的事件处理器总和一个描述符相关。它使用描述符来识别事件、识别应用程序提供的服务。
- **Reactor 管理器（reactor）**：定义了一些接口，用于应用程序控制事件调度，以及应用程序注册、删除事件处理器和相关的描述符。它是事件处理器的调度核心。 Reactor管理器使用同步事件分离器来等待事件的发生。一旦事件发生，Reactor管理器先是分离每个事件，然后调度事件处理器，最后调用相关的模 板函数来处理这个事件。

[图片上传失败...(image-740c74-1538122299754)]

可以看出，是Reactor管理器并不是应用程序负责等待事件、分离事件和调度事件。Reactor并没有被具体的事件处理器调度，而是管理器调度具体的事件处理器，由事件处理器对发生的事件作出处理，这就是Hollywood原则。应用程序要做的仅仅是实现一个具体的事件处理器，然后把它注册到Reactor管理器中。接下来的工作由管理器来完成：如果有相应的事件发生，Reactor会主动调用具体的事件处理器，由事件处理器对发生的事件作出处理。

##### 为什么使用Reactor

有了I/O复用，有了epoll已经可以使服务器并发几十万连接的同时，维持高TPS了，难道这还不够吗？

答案是，技术层面足够了，但在软件工程层面却是不够的。

程序使用IO复用的难点在哪里呢？

1个请求可能由多次IO处理完成，但相比传统的单线程完整处理请求生命期的方法，IO复用在人的大脑思维中并不自然，因为，程序员编程中，处理请求A的时候，假定A请求必须经过多个IO操作A1-An（两次IO间可能间隔很长时间），每经过一次IO操作，再调用IO复用时，IO复用的调用返回里，非常可能不再有A，而是返回了请求B。即请求A会经常被请求B打断，处理请求B时，又被C打断。这种思维下，编程容易出错。

**在程序中：**
 某一瞬间，服务器共有10万个并发连接，此时，一次IO复用接口的调用返回了100个活跃的连接等待处理。先根据这100个连接找出其对应的对象，这并不难，epoll的返回连接数据结构里就有这样的指针可以用。接着，循环的处理每一个连接，找出这个对象此刻的上下文状态，再使用read、write这样的网络IO获取此次的操作内容，结合上下文状态查询此时应当选择哪个业务方法处理，调用相应方法完成操作后，若请求结束，则删除对象及其上下文。

这样，我们就陷入了**面向过程编程**方法之中了，在面向应用、快速响应为王的移动互联网时代，这样做早晚得把自己玩死。我们的主程序需要关注各种不同类型的请求，在不同状态下，对于不同的请求命令选择不同的业务处理方法。这会导致随着请求类型的增加，请求状态的增加，请求命令的增加，**主程序复杂度快速膨胀**，导致维护越来越困难，苦逼的程序员再也不敢轻易接新需求、重构。

反应堆是解决上述软件工程问题的一种途径，它也许并不优雅，开发效率上也不是最高的，但其执行效率与面向过程的使用IO复用却几乎是等价的，所以，无论是nginx、memcached、redis等等这些高性能组件的代名词，都义无反顾的一头扎进了反应堆的怀抱中。

反应堆模式可以在软件工程层面，将事件驱动框架分离出具体业务，将不同类型请求之间用OO的思想分离。通常，反应堆不仅使用IO复用处理网络事件驱动，还会实现定时器来处理时间事件的驱动（请求的超时处理或者定时任务的处理）

##### Reactor的几种模式

###### 1 单线程模式

这是最简单的单Reactor单线程模型。Reactor线程是个多面手，负责多路分离套接字，Accept新连接，并分派请求到处理器链中。该模型适用于处理器链中业务处理组件能快速完成的场景。不过这种单线程模型不能充分利用多核资源，所以实际使用的不多。



![img](https:////upload-images.jianshu.io/upload_images/9166160-7d6414c45138fe3c.png?imageMogr2/auto-orient/strip|imageView2/2/w/625/format/webp)

image

###### 2 多线程模式（单Reactor）

该模型在事件处理器（Handler）链部分采用了多线程（线程池），也是后端程序常用的模型。



![img](https:////upload-images.jianshu.io/upload_images/9166160-1041bbf6750f4023.png?imageMogr2/auto-orient/strip|imageView2/2/w/656/format/webp)

image

###### 3 多线程模式（多个Reactor）

比起第二种模型，它是将Reactor分成两部分，mainReactor负责监听并accept新连接，然后将建立的socket通过多路复用器（Acceptor）分派给subReactor。subReactor负责多路分离已连接的socket，读写网络数据；业务处理功能，其交给worker线程池完成。通常，subReactor个数上可与CPU个数等同。



![img](https:////upload-images.jianshu.io/upload_images/9166160-ae82e4c112052622.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/632/format/webp)

image

#### Proacotr模型

Proactor是和异步I/O相关的。

##### 比较

以读操作为例：
 在Reactor（同步）中实现读：

- 注册读就绪事件和相应的事件处理器
- 事件分离器等待事件
- 事件到来，激活分离器，分离器调用事件对应的处理器。
- 事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权。

Proactor（异步）中的读：

- 处理器发起异步读操作（注意：操作系统必须支持异步IO）。在这种情况下，处理器无视IO就绪事件，它关注的是完成事件。
- 事件分离器等待操作完成事件
- 在分离器等待过程中，操作系统利用并行的内核线程执行实际的读操作，并将结果数据存入用户自定义缓冲区，最后通知事件分离器读操作完成。
- 事件分离器呼唤处理器。
- 事件处理器处理用户自定义缓冲区中的数据，然后启动一个新的异步操作，并将控制权返回事件分离器。

### JAVA NIO

在 JDK 1. 4 中 新 加入 了 NIO( New Input/ Output) 类, 引入了一种基于通道和缓冲区的 I/O 方式，它可以使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆的 DirectByteBuffer 对象作为这块内存的引用进行操作，避免了在 Java 堆和 Native 堆中来回复制数据。

要想了解JAVA NIO，首先得掌握以下几个关键的概念：Buffer， Channel， Selector

#### Buffer

前面提到了，JAVA NIO是一种基于缓冲区的I/O 方式，这是因为当一个链接建立完成后，IO的数据未必会马上到达，当数据到达时，为了不低效的让线程阻塞等待，可以预先把数据写入缓冲区，再由缓冲区交给线程，因此线程无需阻塞地等待IO。

#### Channel

> 通道是 I/O 传输发生时通过的入口，而缓冲区是这些数 据传输的来源或目标。

对于离开缓冲区的传输，您想传递出去的数据被置于一个缓冲区，被传送到通道。对于传回缓冲区的传输，一个通道将数据放置在您所提供的缓冲区中。
 可以理解为在NIO中：如果想将Data发到目标端，则需要将存储该Data的Buffer，写入到目标端的Channel中，然后再从Channel中读取数据到目标端的Buffer中。

#### Selector

通道和缓冲区的机制，使得线程无需阻塞地等待IO事件的就绪，但是总是要有人来监管这些IO事件。这个工作就交给了selector来完成，这就是所谓的同步。

Selector允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用Selector就会很方便。

要使用Selector，得向Selector注册Channel，然后调用它的select()方法。这个方法会一直阻塞到某个注册的通道有事件就绪，这就是所说的轮询。一旦这个方法返回，线程就可以处理这些事件。

Selector中注册的感兴趣事件有：

- OP_ACCEPT
- OP_CONNEC
- OP_READ
- OP_WRITE

很关键的一点是Selector为前文所述的Reactor模型提供了基础，因此常常会将Selector优化成Reactor模型

### NIO&epoll：

可以这么理解NIO是JAVA的IO模型，而epoll是Linux内核的IO模型。它们之间有很深的因缘，因为从实现方式来看，它们其实是很相似的，都是基于“通道”和缓冲区的，也有selector，只是在epoll中，通道实际上是操作系统的“管道”。和NIO不同的是，NIO中，解放了线程，但是需要由selector阻塞式地轮询IO事件的就绪；而epoll中，IO事件就绪后，会自动发送消息，通知selector：“我已经就绪了。”可以认为，Linux的epoll是一种效率更高的NIO。

